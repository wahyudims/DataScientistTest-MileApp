# DataScientistTest-MileApp
This is a project that I've done in order to fulfill the requirement process in MileApp. This project tells us about a dataset of delivery task for 10 days, from this data I'd like to get some insights which consist of Task Duration Analysis, Task Status Analysis, Cash on Delivery Analysis, Geolocation Analysis, Task Assignment Analysis, Product Analysis, and a machine learning model to predict how long the task duration will be.

## Context
This dataset is about a sample of task delivery in 10 days. This dataset consists of 18 columns that contain various features from multiple dimensions: from location origin and destination, time started and completed, task status, the workers who did the delivery, and the product weight.

## Task
Create analysis based on the data and make a model that could predict how long the task duration will be.

## Dataset Info
The dataset consists of 18 columns. Below are the explanation of each columns.
| Field | Description |
|-----------------|-----------------|
| taskId | Unique identifier for the task that generated by system. |
| taskCreatedTime | Time at when the task was created. |
| taskCompletedTime | Time at when the task was completed. |
| taskAssignedTo | Worker that doing the task. |
| taskLocationDone | Coordinate of where the task was completed. |
| flow | Time at when the task was created. |
| cod | Time at when the task was created. |
| cod.amount | Time at when the task was created. |
| cod.received | COD has been received or not. |
| UserVar | Contains more specified data, in this case the 'UserVar' is about delivery task data. |
| UserVar.taskStatus | Delivery status code. |
| UserVar.taskStatusLabel | Delivery status label. |
| UserVar.taskDetailStatus | Detailed delivery status code. |
| UserVar.taskDetailStatusLabel | Detailed delivery status label. |
| UserVar.branch_origin | Branch code of the origin. |
| UserVar.branch_dest | Branch code of the destination. |
| UserVar.weight | Weight of the package. |

## Project Outline
The following are a few steps in the modeling technique used on this project:
1. Import Modules/Packages.
2. Import Dataset.
3. Exploratory Data Analysis (EDA).
4. Data Cleaning
5. Data Preprocessing.
6. Prediction Modelling.
7. Conclusion & Recommendation.

## Load the dataset
The dataset is in JSON format. To load the data we will use json.load() function and then normalize it using pandas.json_normalize(). The final dataframe will consists of total rows of 8,334 rows and 18 columns.

## Exploratory Data Analysis (EDA)
Before we jump into the EDA. First we will set several deep dive question.
 * How is the distribution of the task duration?
 * How is the the distribution of task status?
 * How many transaction did Cash on Delivery used?
 * How is geolocation distribution in task delivery?
 * How is the efficiency of the workers in doing task delivery?
 * How is the characteristics of the products that were ordered in task delivery?

Let's jump to each deep dive question.
 
* How is the distribution of the task duration? <br>
Let's see how the task duration is distributed. <br><br>
<img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/80ed9fff-f45c-4f8f-8e04-1d89669a43fd" width="700"> <br><br>

From both of those graphs. We could get several insights:
1. Task Completion Efficiency: The right-skewed distribution means the majority of the tasks are completed quickly which means potentially higher customer satisfaction.
2. Resources efficiency: Since most of the task are completed quickly. It means that current resources could handle the task pretty efficiently.
3. Indetifying Outliers: There are still several outliers which took longer hours compared to the average, ranging from 4 all the way up to 12 hours. These outliers need to be analysed separately to know what the problem was? Why did it take longer to completed compared to the average?.<br>

Next I'll try to analyze the outlier in term of task duration.<br>

  * Workers with longest duration to complete a task.<br>
    ![image](https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/7222efcd-3a56-4d18-82eb-340acd0936b5)<br>
        Here are top 5 workers with the high task duration in completing delivery which are 'affectedKitten8', 'soreTomatoe9', 'emptyIcecream6',
       'wornoutEggs4', 'peacefulVenison0'.<br><br>
  * Destination with longest duration to complete a task.<br>
    ![image](https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/2ceee7d8-6fa8-43db-af68-388ec675b944)<br>
        Here are top 5 destination with the highest count of long task duration which are Magelang, Bandung, Cilegon, Cengkareng, dan Semarang.<br><br>
  * Weight product in task with long duration.<br>
    ![image](https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/4e01feec-c612-4441-9015-f8026880464f)<br>
        As we can see the data that falls in outlier zone tends to have smaller standard deviation than the counterpart. It means that the weight of the product that has longer task duration tends to be in lower weight.<br><br>

* How is the the distribution of task status? <br>
Let's see how the task status is distributed. <br><br>
<img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/974f065a-db4d-4bdc-a556-55061cb6956c"> <br><br>

As we can see from the graph above, we can coclude several insights:
1. Most tasks are marked as "success" indicates a high completion rate. It suggests that the majority of tasks are being successfully completed within the expected timeframe. This can be a positive indicator of efficiency and effectiveness in task management.
2. The higher number of tasks marked as "Success" implies that the workflow or processes in place are generally efficient. It could means that the workflow is efficient enough that most of the tasks are already done.
3. There are 28.3% tasks marked as "Failed". This number is a little bit high for a "Failed" task so we need to do further analysis of why those tasks can be failed.<br>

  * Task Failed Analysis<br><br>
    <img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/5f953e20-707b-4cae-9de7-276d29667866" width="700"> <br>
    From weight perspective. The task that was failed has lower standard deviation which means product with lower weight more likely to be fail than the higher weight.<br><br>
    <img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/2551b9b9-9980-401a-ae72-ecdcb91d0ad4" width="700"> <br>
    As we can see there are several reasons why the delivery failed. Most of it was because of U12 (Misroute), followed by U01 (Alamat tidak lengkap), and U05 (New Address).<br><br>

* How many transaction did Cash on Delivery used?<br><br>
<img src="![image](https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/c363b0c2-4a78-4ff5-9b84-43cf094d3a18)" width="700"> <br>
We could see that the majority of customers prefer non-CoD payment methods.

* How is geolocation distribution in task delivery?<br><br>
<img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/0a54c0d5-cd05-4071-b6c0-25bb4cec97e2" width="700"> <br>
The graph above shows the transaction count for each branches and the cumulative percentage of the transaction. According to The Pareto principle, also known as the 80/20 rule, suggests that roughly 80% of the effects come from 20% of the causes. So by focusing more on branches that account for 80% of the transactions, we can allocate our resources and efforts more efficiently. Instead of distributing our resources evenly across all branches, we can prioritize those key branches that generate the majority of the transactions. This approach allows us to maximize our impact and optimize our operations based on the Pareto principle.<br><br>

* How is the efficiency of the workers in doing task delivery?<br>
Top 5 workers with highest task assignment.<br><br>
![image](https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/08e4e430-d42d-47cd-a24f-8249875ab742)<br>
Task assignment distribution.<br><br>
<img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/5722049a-eaf2-4401-bae3-42f0011c7031" width="700"> <br>
As we can see there is an imbalance in term of workload. There are several workers who are assigned to a lot of task even more than hundred task while the other only has as little as 1 task. This means that there are so many underutilized workers. We could try to distribute the workload among the workers thus increasing the efficiency of their work.<br>
  * Task assignment efficiency.<br><br>
  ![image](https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/28052b9a-7749-4c88-b966-e39fd6808ebc)<br>
  As we can see 5 top performers with the most assigned task isn't the top performers in term of Task Duration. It could means that they are overloaded with their task thus make them perform poorly. That's why we need to distribute the workload more balance so each workers can perform more efficient.<br><br>
  
* How is the characteristics of the products that were ordered in task delivery?<br><br>
<img src="https://github.com/wahyudims/DataScientistTest-MileApp/assets/89758536/14be7c2b-8d22-4415-8b04-04d8b80d0b93" width="700"> <br>
From the graph above we could say that almost all of the product ordered are around 0-20 kg or lb while the biggest product ordered is weighing around 200 kg or lb.

## Machine Learning Model

### Data Cleaning
Most of the missing values are caused by the task still being ongoing. Because we only need the task that had been done so we will drop data with missing value.

### Feature Selection
From my knowledges, There are several factors that can affect how long the task duration will be. Based on this dataset, several columns that I will use as the features are "taskAssignedTo", "taskLocationDone.lon", "taskLocationDone.lat", "UserVar.weight", and "UserVar.branch_origin".

### Preprocessing Categorical Variable
The dataset we use has two categorical features which are "taskAssignedTo" and "UserVar.branch_origin". Because both of the features have too many values, using one hot encoder isn't visible. Instead we will use target encoder.

### Classification Prediction Modelling
* The dataset split into 80:20 with 80% is training data and 20% is test data
* We will use Mean Squared Error and R-squared Score as the metrics.
* I used Linear Regression as baseline model, then compared it with Decision Tree, Random Forest, and XGBoost. The best algorithm is Random Forest which gives us Mean Squared Error of 0.64 and R-squared Score of 0.79. MSE of 0.64 means that the predicted task durations deviate from the actual durations by approximately 0.64 squared units and R-squared Score of 0.79 means approximately 79% of the variance in task durations can be explained by the independent variables included in our model.<br><br>

| Model Algorithm | MSE | R-squared Score |
|----------|----------|----------|
| Linear Regression   | 1.45  | 0.51   |
| Decision Tree   | 0.73  | 0.75  |
| Random Forest   | 0.64  | 0.79   |
| XGBoost   | 0.72  | 0.76   |

## Conclusion
* Most tasks are marked as "success" indicates a high completion rate. It suggests that the majority of tasks are being successfully completed within the expected timeframe. This can be a positive indicator of efficiency and effectiveness in task management.<br>
* Since most of the task are completed quickly. It means that current resources could handle the task pretty efficiently.<br>
* Top 5 workers who took longer time to complete their tasks are 'affectedKitten8', 'soreTomatoe9', 'emptyIcecream6', 'wornoutEggs4', and 'peacefulVenison0'. We could reevaluate their performance, give them some training about time management, or distributed task more evenly to increase their efficiency.<br>
* top 5 destination with the highest count of long task duration are Magelang, Bandung, Cilegon, Cengkareng, and Semarang. We could try to reevaluate the workflow in those branches maybe there are some mismanagement that could make the delivery task take much longer time.<br>
* Most tasks are marked as "success" indicates a high completion rate. It suggests that the majority of tasks are being successfully completed within the expected timeframe. This can be a positive indicator of efficiency and effectiveness in task management.
* From weight perspective. The task that was failed has lower standard deviation which means product with lower weight more likely to be fail than the higher weight.
* Most of the reasons the delivery failed were because of U12 (Misroute), followed by U01 (Alamat tidak lengkap), and U05 (New Address).
* The majority of customers prefer non-COD payment methods. This could indicate that alternative payment options, such as online payments or credit/debit card transactions, are more popular and convenient for customers. It suggests that customers may prefer the convenience and security provided by non-COD payment methods.
* From geolocation analysis we have several branches that contribute to 80% of the transaction. We could focus our effort more in these branches which allow us to maximize our impact and optimize our operations
* There are so many underutilized performers where the overutilized performers aren't top performers in term of task duration efficiency. It means that we need to distribute the task more evenly so everyone can work more efficiently.
* Most of the product ordered are weighing around 0-20 kg / lb while there are some products weighing as high as 200 kg / lb.
* The best algorithm to predict task duration is Random Forest which gives us Mean Squared Error of 0.64 and R-squared Score of 0.79. By using this machine learning model it could improve resource allocation. Accurate task duration prediction helps in efficient resource allocation. It allows businesses to allocate the right amount of resources, such as manpower, equipment, and materials, to complete tasks within the estimated time frame. It also could help us to enhance our planning and scheduling. With task duration prediction, businesses can better plan and schedule their operations. They can estimate when tasks will be completed, allowing for effective scheduling of subsequent tasks or projects.
